---
title: "Generated Report on Revision Analysis"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: 
  html_document:
    theme: cerulean
    toc: true
    toc_float: true
  pdf_document:
    toc: true
always_allow_html: true
bibliography: revision_analysis.bib
nocite: '@*'
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Content in a nutshell

Descriptive statistics provide basic information about the extend of revisions. 

Parametric tests enables the users to detect potential bias in revisions. In addition, they intend to measure whether preliminary estimates are efficient. This being the case, revisions should not be predictable in any way. Keep in mind that even though biases and efficiency are tested on an interval of time, they may, in reality, be changing overtime.

For some parametric tests, transformation may be important to avoid misleading results. By default, the decision to differentiate vintages is performed automatically based on unit root and co-integration tests ^[More specifically, we focus on the augmented Dickey-Fuller (ADF) test to test the presence of unit root and, for cointegration, we proceed to an ADF test on the residuals of an OLS regression between the two vintages. See for example @RevAnTechreport2 for a full description of those tests.] whose results are made available in the output of the `revision_analysis()` function. However, the choice of log-transformation is left to the users based on their knowledge of the series and the diagnostics of the various tests. By default, no log-transformation is considered. 


# Descriptive statistics

```{r descriptivetable, echo = FALSE}
rslt |> build_table(type = "stats-desc")
```

```{r plot revisions, echo = FALSE}
if (add_plot) {
    stats::ts.plot(revisions, gpars = list(xlab = "", ylab = "", col = seq_len(ncol(revisions)), type = "h", lwd = 2))
    graphics::legend("topleft", bty = "n", lty = 1, lwd = 2, col = seq_len(ncol(revisions)), legend = colnames(revisions))
    graphics::title(main = "Revisions size")
}
```


# Main Results

```{r summarytable, echo = FALSE}
rslt |> build_table(type = "summary")
```

# Tests description

## I. Relevancy

### Theil’s Inequality Coefficient

In the context of revision analysis, Theil's inequality coefficient, also known as Theil's U, provides a measure of the accuracy of a set of preliminary estimates (P) compared to a latter version (L). There exists a few definitions of Theil's statistics leading to different interpretation of the results. In this package, two definitions are considered. The first statistic, U1, is given by

$$
U_1=\frac{\sqrt{\frac{1}{n}\sum^n_{t=1}(L_t-P_t)^2}}{\sqrt{\frac{1}{n}\sum^n_{t=1}L_t^2}+\sqrt{\frac{1}{n}\sum^n_{t=1}P_t^2}}
$$

U1 is bounded between 0 and 1. The closer the value of U1 is to zero, the better the forecast method. However, this classic definition of Theil's statistic suffers from a number of limitations. In particular, a set of near zero preliminary estimates would always give a value of the U1 statistic close to 1 even though they are close to the latter estimates.

The second statistic, U2, is given by

$$
U_2=\frac{\sqrt{\sum^n_{t=1}\left(\frac{P_{t + 1}-L_{t + 1}}{L_t}\right)^2}}{\sqrt{\sum^n_{t=1}\left(\frac{L_{t + 1}-L_t}{L_t}\right)^2}}
$$

The interpretation of U2 differs from U1. The value of 1 is no longer the upper bound of the statistic but a threshold above (below) which the preliminary estimates is less (more) accurate than the naïve random walk forecast repeating the last observed value (considering $P_{t + 1}=L_t$). Whenever it can be calculated ($L_t \neq 0 ~\forall t$), the U2 statistic is the preferred option to evaluate the relevancy of the preliminary estimates.


## II. Bias

A bias in preliminary estimates may indicate inaccurate initial data or inefficient compilation methods. However, we must be cautious if a bias is shown to be statistically significant and we intend to correct it. Biases may change overtime, so we might correct for errors that no longer apply. Over a long interval, changes in methodology or definitions may also occur so that there are valid reasons to expect a non-zero mean revision.

### II.1_2. T-test and Augmented T-test

To test whether the mean revision is statistically different from zero for a sample of n, we employ a conventional t-statistic

$$
t=\frac{\overline{R}}{\sqrt{s^2/n}}
$$

If the null hypothesis that the bias is equal to zero is rejected, it may give an insight of whether a bias exists in the earlier estimates. 

The t-test is equivalent to fitting a linear regression of the revisions on only a constant (i.e. the mean). Assumptions such as the gaussianity of the revisions are implied. One can release the assumption of no autocorrelation by adding it into the model. Hence we have

$$
R_t=\mu+\varepsilon_t, ~~t=1,...,n
$$

And the errors are thought to be serially correlated according to an AR(1) model, that is

$$
\varepsilon_t=\gamma\varepsilon_t+u_t, ~~with~ |\gamma|<1 ~and ~u_t \sim{iid} 
$$

The auto-correlation in the error terms reduces the number of independent observations (and the degrees of freedom) by a factor of $n\frac{(1-\gamma)}{(1+\gamma)}$ and thus, the variance of the mean should be adjusted upward accordingly.

Hence, the Augmented t-test is calculated as

$$
t_{adj}=\frac{\overline{R}}{\sqrt{\frac{s^2(1+\hat{\gamma})}{n(1-\hat{\gamma})}}}
$$

where

$$
\hat{\gamma}=\frac{\sum^{n-1}_{t=1}(R_t-\overline{R})(R_{t + 1}-\overline{R})}{\sum^n_{t=1}(R_t-\overline{R})^2}
$$

### II.3. Slope and drift

We assume a linear regression model of a latter vintage (L) on a preliminary vintage (P) and estimate the intercept (drift) $\beta_0$ and slope coefficient $\beta_1$ by OLS. The model is

$$
L_t=\beta_0+\beta_1P_t+\varepsilon_t
$$

While the (augmented) t-test on revisions only gives information about mean bias, this regression enables to assess both mean and regression bias. A regression bias would occur, for example, if preliminary estimates tend to be too low when latter estimates are relatively high and too high when latter estimates are relatively low. In this case, this may result in a negative value of the intercept and a value of $\beta_1>1$. To evaluate whether a mean or regression bias is present, we employ a conventional t-test on the parameters with the null hypothesis $\beta_0 = 0$ and $\hat{\beta}_1 = 1$.  

Recall that OLS regressions always come along with rather strict assumptions. Users should check the diagnostics and draw the necessary conclusions from them. 

## III. Efficiency

Efficiency tests evaluate whether preliminary estimates are "efficient" forecast of the latter estimates. If all information were used efficiently at the time of the preliminary estimate, revisions should not be predictable and therefore neither be correlated with preliminary estimates or display any relationship from one vintage to another. This section focuses on these two points. Predictability of revisions is tested even further in the Orthogonality and SignalVSNoise sections.

### III.1. Regression of revisions on previous estimates

We assume a linear regression model of the revisions (R) on a preliminary vintage (P) and estimate the intercept $\beta_0$ and slope coefficient $\beta_1$ by OLS. The model is

$$
R_t=\beta_0+\beta_1P_t+\varepsilon_t, ~~t=1,...,n
$$

If revisions are affected by preliminary estimates, it means that those are not efficient and could be improved. We employ a conventional t-test on the parameters with the null hypothesis $\beta_0 = 0$ and $\beta_1 = 0$. Diagnostics on the residuals should be verified.

### III.2. Regression of revisions from latter vintages on revisions from the previous vintages

We assume a linear regression model of the revisions between latter vintages ($R_v$) on the revisions between the previous vintages ($R_{v-1}$) and estimate the intercept $\beta_0$ and slope coefficient $\beta_1$ by OLS. The model is

$$
R_{v,t}=\beta_0+\beta_1R_{v-1,t}+\varepsilon_t, ~~t=1,...,n
$$

If latter revisions are predictable from previous revisions, it means that preliminary estimates are not efficient and could be improved. We employ a conventional t-test on the parameters with the null hypothesis $\beta_0 = 0$ and $\beta_1 = 0$. Diagnostics on the residuals should be verified.


## IV. Orthogonality

Orthogonality tests evaluate whether revisions from older vintages affect the latter revisions. This section also includes autocorrelation and seasonality tests for a given set of revisions. If there is significant correlation in the revisions for subsequent periods, this may witness some degree of predictability in the revision process.          

### IV.1. Regression of latter revisions (Rv) on previous revisions (Rv_1, Rv_2,...Rv_p)

We assume a linear regression model of the revisions from latter vintages ($R_v$) on the revisions from p previous vintages ($R_{v-1}, ..., R_{v-p}$) and estimate the intercept $\beta_0$ and slope coefficients of $\beta_1, ..., \beta_p$ by OLS. The model is

$$
R_{v,t}=\beta_0+\sum^p_{i=1}\beta_{i}R_{v-i,t}+\varepsilon_t, ~~t=1,...,n
$$

If latter revisions are predictable from previous revisions, it means that preliminary estimates are not efficient and could be improved. We employ a conventional t-test on the intercept parameter with the null hypothesis $\beta_0 = 0$ and a F-test to check the null hypothesis that $\beta_1 = \beta_2=...=\beta_p=0$. Diagnostics on the residuals should be verified

### IV.2. Regression of latter revisions (Rv) on revisions from a specific vintage (Rv_k)

We assume a linear regression model of the revisions from latter vintages ($R_v$) on the revisions from a specific vintage ($R_{v-k}$) and estimate the intercept $\beta_0$ and slope coefficient $\beta_1$ by OLS. The model is

$$
R_{v,t}=\beta_0+\beta_1R_{v-k,t}+\varepsilon_t, ~~t=1,...,n
$$

If latter revisions are predictable from previous revisions, it means that preliminary estimates are not efficient and could be improved. We employ a conventional t-test on the parameters with the null hypothesis $\beta_0 = 0$ and $\beta_1 = 0$. Diagnostics on the residuals should be verified.

### IV.3. Test of autocorrelations in revisions 

To test whether autocorrelation is present in revisions, we employ the Ljung-Box test. The Ljung-Box test considers together a group of autocorrelation coefficients up to a certain lag k and is therefore sometimes referred to as a portmanteau test. For the purpose of revision analysis, as we expect a relatively small number of observations, we decided to restrict the number of lags considered to $k=2$. Hence, the users can also make the distinction with autocorrelation at seasonal lags (see seasonality tests below).     

The null hypothesis is no autocorrelation. The test statistic is given by

$$
Q=n(n+2)\sum^k_{i=1}\frac{\hat{\rho}_i^2}{n-i}
$$

where n is the sample size, $\hat{\rho}_i^2$ is the sample autocorrelation at lag in and $k=2$ is the number of lags being tested. Under the null hypothesis, $Q\sim\chi^2(k)$.

If Q is statistically different from zero, the revision process may be locally biased in the sense that latter revisions are related to the previous ones. 

### IV.4. Test of seasonality in revisions

To test whether seasonality is present in revisions, we employ two tests: the parametric QS test and the non-parametric Friedman test. Note that seasonality tests are always performed on first-differentiated series to avoid misleading results.

The QS test is a variant of the Ljung-Box test computed on seasonal lags, where we only consider positive auto-correlations

$$
QS=n(n+2)\sum^k_{i=1}\frac{\left[max(0,\hat{\gamma}_{i.l})\right]^2}{n-i.l}
$$

where $k=2$, so only the first and second seasonal lags are considered. Thus, the test would checks the correlation between the actual observation and the observations lagged by one and two years. Note that $l=12$ when dealing with monthly observations, so we consider the auto-covariances $\hat{\gamma}_{12}$ and $\hat{\gamma}_{24}$ alone. In turn $k=4$ in the case of quarterly data.

Under the null hypothesis of no autocorrelation at seasonal lags, $QS\sim \chi_{modified}^2(k)$. The elimination of negative correlations calls indeed for a modified $\chi^2$ distribution. This was done using simulation techniques.

The Friedman test requires no distributional assumptions. It uses the rankings of the observations. It is constructed as follows. Consider first the matrix of data $\{x_{ij}\}_{nxk}$ with n rows (the blocks, i.e. the number of years in the sample), k columns (the treatments, i.e., either 12 months or 4 quarters, depending on the frequency of the data). The data matrix needs to be replaced by a new matrix $\{r_{ij}\}_{nxk}$, where the entry $r_{ij}$ is the rank of $x_{ij}$ within the block i. The test statistic is given by

$$
Q=\frac{n\sum^k_{j=1}(\tilde{r}_{.j}-\tilde{r})^2}{\frac{1}{n(k-1)}\sum^n_{i=1}\sum^k_{j=1}(r_{ij}-\tilde{r})^2}
$$

The denominator represents the variance of the average ranking across treatments j relative to the total. Under the null hypothesis of no (stable) seasonality, $Q\sim \chi^2(k-1)$.

As for non-seasonal autocorrelation tests at lower lags, if QS and Q are significantly different from zero, the revision process may be locally biased in the sense that latter revisions are related to the previous ones.


## V. Signal vs Noise

Regression techniques can also be used to determine whether revisions should be classified as ‘news’ or ‘noise’. Those are also closely related to the notion of efficiency developed earlier. 

If the correlation between revisions and preliminary estimates is significantly different from zero, this implies that we did not fully utilize all the information available at the time of the preliminary estimates. In that sense, we would conclude that the preliminary estimates could have been better and that revisions embody noise. The model to test whether revisions are "noise" is similar to the first model established earlier to test efficiency: 

$$
R_t=\beta_0+\beta_1P_t+\varepsilon_t, ~~t=1,...,n
$$

We employ a F-test on the parameters to test jointly that $\beta_0 = 0$ and $\beta_1 = 0$. If the null hypothesis is rejected, this suggest that revisions are likely to include noise. Diagnostics on the residuals should be verified.

On the other hand, revisions should be correlated to the latter estimates. If this is the case, it means that information which becomes
available between the compilation of the preliminary and the latter estimates are captured in the estimation process of the latter estimates. In that sense, the revision process is warranted and we conclude that revisions embody news. The model to test whether revisions are "news" is

$$
R_t=\beta_0+\beta_1L_t+\varepsilon_t, ~~t=1,...,n
$$

We employ a F-test on the parameters to test jointly that $\beta_0 = 0$ and $\beta_1 = 0$. If the null hypothesis is rejected, this is a good thing as it suggests that revisions incorporate news. Note that even if we reject the null hypothesis and conclude that revisions incorporate news, it does not necessarily mean that revisions are efficient as those might still be predicted by other variables not included in the estimation process.


# References
